# Tech Challenge 2 (Fase 2): Pipeline Batch Bovespa: Ingest√£o e Arquitetura de dados


**Tech Challenge** √© um projeto que re√∫ne a aplica√ß√£o dos conhecimentos adquiridos em todas as disciplinas de uma fase da Especializa√ß√£o em Machine Learning Engineering da FIAP PosTech.

Para o Tech Challenge 2, o desafio proposto foi o seguinte:

> üì¢ **Problema:** construa um pipeline de dados completo para **Extrair, Processar e Analisar dados do preg√£o da B3 (IBovespa)**, utilizando AWS S3, Glue, Lambda e Athena. Para acessar os dados, obrigatoriamente acessar o link: [Carteira Te√≥rica o IBovespa](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br)

Para este desafio as entregas devem ser realizadas utilizando tecnologias da **Amazon Cloud** e atender aos seguintes **Requisitos/objetivos**:

‚Ä¢ **Requisito 1:** realizar o scrap de dados do site da B3 (IBovespa), extraindo dados do preg√£o (dados brutos).

‚Ä¢ **Requisito 2:** os dados brutos devem ser ingeridos no S3 em formato parquet com parti√ß√£o di√°ria.

‚Ä¢ **Requisito 3:** o Bucket S3 deve acionar uma Lambda, que por sua vez ir√° chamar um job de ETL no Glue.

‚Ä¢ **Requisito 4:** a Lambda pode ser em qualquer linguagem. Ela apenas dever√° iniciar o job Glue.

‚Ä¢ **Requisito 5:** o job Glue deve ser feito no modo visual. Este job deve conter as seguintes transforma√ß√µes obrigat√≥rias:

5.a: Realizar agrupamento num√©rico, sumariza√ß√£o, contagem ou soma;

5.b: Renomear duas colunas existentes, al√©m das colunas de agrupamento;

5.c: Realizar um c√°lculo com campos de data; por exemplo, poder ser dura√ß√£o, compara√ß√£o ou diferen√ßa entre datas.

‚Ä¢ **Requisito 6:** os dados refinados no job Glue devem ser salvos no formato parquet em uma pasta chamada REFINED, particionados por data e pelo nome ou abrevia√ß√£o da a√ß√£o do preg√£o.

‚Ä¢ **Requisito 7:** o job Glue deve automaticamente catalogar o dado no Glue Catalog e criar uma tabela no banco de dados default do Glue Catalog.

‚Ä¢ **Requisito 8:** os dados devem estar dispon√≠veis e leg√≠veis no Athena.

‚Ä¢ **Requisito 9:** (OPCIONAL) construir um notebook no Athena para realizar uma visualiza√ß√£o gr√°fica dos dados ingeridos.

‚Ä¢ **Requisito 10:** (OPCIONAL) construir uma Pipeline Stream Bitcoin, conforme arquitetura de refer√™ncia fornecida.

## üìå Objetivos

- Elaborar a **Arquitetura do projeto**, demonstrando todas as fases da pipeline;
- Implementar as tecnologias para o atendimento dos requisitos da Pipeline de Ingest√£o de Dados da B3;
- Documentar o projeto de forma a permitir a sua reprodu√ß√£o;
- Disponibilizar a documenta√ß√£o em um reposit√≥rio no **GitHub**.

## Poss√≠veis dores

- Falta de automa√ß√£o na obten√ß√£o dos dados bem como seu tratamento;
- Falta de padroniza√ß√£o para acesso aos dados bem como tipos de retornos e formatos mais adequados para consumo na produ√ß√£o de consultas e analytics;
- Suporte e documenta√ß√£o insuficientes;
- Redund√¢ncia de dados em hist√≥rico confi√°vel, com fonte de dados pr√≥pria;
- Baixa capacidade de an√°lise em quest√µes relevantes para o usu√°rio final.

## Proposta de solu√ß√£o

Em face ao desafio proposto, algumas funcionalidades propostas (stages) para a Pipeline Batch Bovespa:

- Ingest√£o de dados: coleta autom√°tica de dados (webscraping) extra√≠dos do site B3 (IBovespa), por meio de script em Lambda trigado pelo Event Bridge;
- Armazenamento de dados: salvamento em formato bruto (RAW), com parti√ß√£o di√°ria, em Bucket S3;
- Processamento: limpeza, transforma√ß√£o e padroniza√ß√£o, usando script em Glue acionado pela Lambda e trigado por Event Bridge;
- Carga final: grava√ß√£o de dado procesado (REFINED), com parti√ß√£o di√°ria, em Bucket S3;
- Agrega√ß√µes e C√°lculos: agrega√ß√µes e c√°lculos, usando script em Glue, para an√°lise do comportamento dos dados;
- Consumo: leitura dos dados refinados e agregados para a produ√ß√£o de relat√≥rios e dashboards, usando Athena e Google Colab.



**Importante**

Toda a implementa√ß√£o foi feita via Terraform, portanto, com o princ√≠pio de **Infrastructure as a Code** e foi documentada neste reposit√≥rio.



### üìÇ Estrutura do projeto

(inserir)

### üî© Arquitetura da solu√ß√£o

A arquitetura da solu√ß√£o foi desenhada com base nos stages necess√°rios ao atendimento de requisitos e consta na pasta de documenta√ß√£o deste reposit√≥rio. [Link para o Diagrama](inserir figura e link)

## Documenta√ß√£o Terraform

```

# terraform-infra-pipeline-aws

# ![logo61](docs/imagens/logo.png) 
  # Sobre o Projeto

**Tech Challenge** √© um projeto que re√∫ne a aplica√ß√£o dos conhecimentos adquiridos em todas as disciplinas na segunda fase da Especializa√ß√£o em Machine Learning Engineering da FIAP PosTech.

Para o Tech Challenge 2, o desafio proposto foi o seguinte:

> üì¢ **Problema:** Construa um pipeline de dados completo para extrair, processar e 
analisar dados do preg√£o da B3, utilizando AWS S3, Glue, Lambda e Athena. 

**Link do site:** [B3](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br)

**Proposta do desafio:**
A proposta do projeto √© criar projeto na plataforma **AWS Cloud** para realizar um fluxo de ETL e consulta nos dados dispon√≠veis no site da B3 :

- Carteira Te√≥rica do IBovespa

A API desenvolvida ser√° utilizada para alimentar uma base de dados que servir√° para o processo de ETL com os servi√ßos da **AWS Cloud**.

## üìå Objetivos
Automatizar o processo de extra√ß√£o, ingest√£o, processamento e an√°lise de dados do preg√£o da B3, utilizando servi√ßos AWS escal√°veis e serverless:
- Extrair (scraping) dados do site da B3.
- Ingerir e armazenar esses dados em um bucket S3 em formato otimizado (parquet), com parti√ß√µes di√°rias.
- Orquestrar o processamento com Lambda e Glue (ETL visual).
- Refinar e transformar os dados com opera√ß√µes de agrega√ß√£o, renomea√ß√£o de colunas e manipula√ß√£o de datas.
- Catalogar automaticamente os dados no Glue Catalog.
- Consult√°-los facilmente com Athena.


## Poss√≠veis dores

üï∏Ô∏è 1. Scraping inst√°vel
  - O site da B3 pode mudar de estrutura ou bloquear requisi√ß√µes automatizadas.
  - Restri√ß√µes de CORS ou autentica√ß√£o podem exigir t√©cnicas avan√ßadas.
  - Manuseio de diferentes formatos (CSV, XLSX, ZIP) √© comum e precisa de tratamento.
üì¶ 2. Ingest√£o no S3
  - Garantir que os dados estejam no formato parquet com parti√ß√£o por data exige cuidado com o nome dos arquivos e a estrutura de diret√≥rios.
  - Falhas de autentica√ß√£o (credenciais IAM mal configuradas) podem impedir uploads.
üîÅ 3. Orquestra√ß√£o via Lambda
  - A fun√ß√£o Lambda precisa lidar com erros e timeouts, especialmente ao chamar o Glue.
  - Limites de tempo da Lambda (m√°x 15 min) podem afetar sua funcionalidade.
üß™ 4. ETL no Glue 
  - Opera√ß√µes como agrega√ß√µes e c√°lculos entre datas exigem compreens√£o do formato dos dados.
  - Glue cobra por tempo de execu√ß√£o ‚Äî transforma√ß√µes ineficientes podem custar mais!
üß≠ 5. Particionamento refinado
    - Particionar por data + nome da a√ß√£o exige manipula√ß√£o correta do schema.
    - Se o nome da a√ß√£o estiver mal formatado, pode quebrar a l√≥gica de parti√ß√£o.
üìö 6. Cataloga√ß√£o e Athena
    - O Glue Catalog pode n√£o registrar corretamente se os arquivos estiverem com metadados inconsistentes.
    - Athena exige que parti√ß√µes estejam bem definidas para conseguir consultas r√°pidas e sem erro.


## Proposta de solu√ß√£o

Em face ao desafio proposto, algumas funcionalidades propostas para a API s√£o:

1. Extra√ß√£o de Dados (Scraping)
- Implementar um script (Python ou Node.js) que realiza o scraping dos dados do preg√£o da B3 a partir do link fornecido.
- Salvar os dados em formato CSV/JSON localmente para tratamento inicial.
- Automatizar execu√ß√£o via CloudWatch Events ou manualmente.
2. Ingest√£o no Amazon S3
- Ap√≥s o scraping, os dados s√£o salvos no Amazon S3 em formato Parquet, particionados por data do preg√£o (YYYY-MM-DD).
3. Disparo da Fun√ß√£o Lambda
- Configurar evento S3 para acionar uma fun√ß√£o Lambda ap√≥s upload dos dados.
- A Lambda, escrita em qualquer linguagem (sugest√£o: Python ou Node.js), apenas inicia o job do AWS Glue (usando boto3 ou SDK da AWS).

4. Transforma√ß√£o com AWS Glue
- Criar job Glue com as seguintes transforma√ß√µes:
    - Agrega√ß√£o: total de negocia√ß√µes por a√ß√£o.
    - Renomear colunas: exemplo, vl_negociado ‚Üí valor_total, qtd_negocios ‚Üí quantidade.
    - C√°lculo temporal: diferen√ßa entre data do preg√£o e data de liquida√ß√£o, ou convers√£o de string para tipo timestamp.
    - Cataloga√ß√£o Autom√°tica no Glue
    - Glue adiciona os dados automaticamente no Glue Catalog sob o database default.
    - Cria√ß√£o da tabela com schema compat√≠vel e parti√ß√µes por data e sigla_acao.
    - Consulta com Amazon Athena
    - Athena habilitado para consulta SQL sobre os dados refinados.
    - Queries para an√°lise de a√ß√µes com maior volume, maior varia√ß√£o, etc.


## üìÇ Estrutura do projeto

```
  
```


## üî© Arquitetura da solu√ß√£o

A arquitetura da solu√ß√£o foi desenhada sob uma abordagem End-to-end e consta na pasta de documenta√ß√£o deste reposit√≥rio. [Link para o Diagrama] 


## Cen√°rios de Machine Learning


## Depend√™ncias

Para o desenvolvimento deste desafio, foram utilizadas as seguintes bibliotecas e frameworks:



## üõ†Ô∏è Instala√ß√£o do projeto local

Clonando o projeto localmente

``` bash
$ git clone 
```

Criando um ambiente virtual

``` bash
$ python -m venv venv
```

Ativando o ambiente virtual

``` bash
$ source venv/Scripts/activate 
```

Instala√ß√£o das dep√™nd√™ncias

``` bash
$ pip install -r api/requirements.txt
```
[Link para baixar requirements.txt] 

Executando o servidor Flask a partir do diret√≥rio raiz do projeto:

``` bash
$ cd api
$ flask run 
```

Ou executar com o debug ativado:

``` bash
$ cd api
$ flask run --debug
```

Testando as consultas localmente via Insomnia a seguir.


## üåê Lambda


## üìú Glue

## Catalog do Glue

## S3 

## Athena




### ‚öôÔ∏è Configura√ß√£o 


## Testes Unit√°rios
- Com as bibliotecas `pytest` e `unittest` instaladas
- Executar o seguinte comando no terminal na raiz do projeto
- 

```bash
$ cd api
$ python -m pytest
```





## V√≠deo de Apresenta√ß√£o no Youtube
Para melhor compreens√£o da entrega, foi produzido um v√≠deo de apresenta√ß√£o que foi publicado no Youtube:


Feito isso, criar o arquivo .github/workflows/terraform.yaml, .github/workflows/develop.yaml, .github/workflows/prod.yaml

## V√≠deo de Apresenta√ß√£o no Youtube
Para melhor compreens√£o da entrega, foi produzido um v√≠deo de apresenta√ß√£o que foi publicado no Youtube:

[Link para a V√≠deo](inserir link)


## ‚úíÔ∏è Autores

| Nome                            |   RM    | Link do GitHub                                      |
|---------------------------------|---------|-----------------------------------------------------|
| Ana Paula de Almeida            | 363602  | [GitHub](https://github.com/Ana9873P)               |
| Augusto do Nascimento Omena     | 363185  | [GitHub](https://github.com/AugustoOmena)           |
| Bruno Gabriel de Oliveira       | 361248  | [GitHub](https://github.com/brunogabrieldeoliveira) |
| Jos√© Walmir Gon√ßalves Duque     | 363196  | [GitHub](https://github.com/WALMIRDUQUE)            |
| Pedro Henrique da Costa Ulisses | 360864  | [GitHub](https://github.com/ordepzero)              |

## üìÑ Licen√ßa


Este projeto est√° licenciado sob a Licen√ßa MIT.  
Consulte o arquivo [license](docs/license/license.txt)  para mais detalhes.